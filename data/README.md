#唐诗词性词频的抓取
本次工作的主要目标是对『吟诗作对』项目进行数据上的优化。

主要进行了三个步骤：
+ 从网络上抓取大量诗词数据
+ 按格式将诗词分类
+ 对诗词正文进行分词操作
+ 统计各词出现的频率

##1、 抓取诗词
首先选定了http://so.gushiwen.org/这个网站作为抓取的目标，这个网站上收录的诗词数量也很可观，词也基本都是主流诗人写的，是很好的数据源。总共抓取了71000首。

观察发现，所有诗歌的正文都在形如http://so.gushiwen.org/view_XXX.aspx的网页上（XXX是数字），这大大简化了抓取操作。

具体抓取和解析代码在「poem-spider.js」中。最后解析出来的结果被保存在「docs/poems.txt」中。

##2、 分类
很简单，采用正则表达式的形式匹配诗文，将五言是、七言诗等分开保存。
具体代码在「poem-selector.js」中。目前解析了七言诗和五言诗，保存在「docs/QiYan.txt」和「docs/WuYan.txt」中

##3、 分词
为了实现词语频率的统计，以及后续词性的分析，首先要进行的是分词操作。
找到了这样一个分词工具：http://thulac.thunlp.org/demo。
下载得到了两个部分的文件：
```
THULAC_lite_java_run.jar
models/
```
jar文件是进行分词的可执行文件，models文件夹包含一些官方提供的训练数据，用以执行分词操作。
针对唐诗，分词效果其实还有提升的空间，比如说下面这样的：
```
神皋福 地 三 秦邑 ， 玉台 金阙九仙家 。 寒光 犹恋 甘泉树 ， 淑景 偏 临建 始 花 。
```
这些词对人来说已经比较难懂了，估计不容易通过训练得到改善。不过好在后面的统计操作，基本能够把这些无意义的分词筛掉。

分词时的命令很简单：
```bash
java -jar thulac\\thulac.jar -seg_only -input data\\WuYan.txt -output data\\Separate_WuYan.txt
```
具体代码在「poem-separate.js」中。
##4、 统计词频
使用字典显然是统计词频的最好的选择。
每读取到一个词就在字典中创建这个条目，或者将这个词的计数增加1。
具体的代码在「high-freq-word.js」中，最后筛选出了所有出现大于1次的词语，结果保存在「docs/high-frequency-word.csv」中。
